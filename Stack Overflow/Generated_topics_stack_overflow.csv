,Id,Title,Text,OriginalText,Document,Topic,Name,Representation,Representative_Docs,Top_n_words,Probability,Representative_document
83,73502727,noise addition weight use Opacus Federated Learning set,"noise addition weight use Opacus Federated Learning set   I plan use Opacus implement differential privacy federate learning model basic doubt would love clear that .    so far understanding go , use Opacus , use optimizer like DPSGD add differential noise batch clientâ€ ™ s dataset â€œlocal trainingâ€. federate learning , train client model â€œlocal epochsâ€ send weight central server aggregation , add differential noise send model weight .    so question be , use DPSGD add noise every single batch every single client dataset local training could add noise local weight send out ? let local training epoch happen simply add noise outbound weight time departure ? miss ?","noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . 
  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€. and in federated learning , we train client model for a few â€œlocal epochsâ€ before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . 
  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? 
","noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . 
  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€. and in federated learning , we train client model for a few â€œlocal epochsâ€ before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . 
  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? 
",7,7_noise_privacy_differential_differential privacy,"['noise', 'privacy', 'differential', 'differential privacy', 'add', 'clip', 'add noise', 'use', 'training', 'model', 'use opacus', 'clipping', 'opacus', 'set', 'train', 'batch', 'happen', 'federate', 'local', 'learning']","['Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . \n I be use TFF in case someone be wonder . \n  I load my datum as : \n \n  and I set Q as sample ratio \n \n  give this I define my dp parameter : \n \n  Noise = 0.5 \n  q = 0.015 \n  n_clients_per_round = int(q*len(train_data.client_id ) ) \n \n  I define my aggregation factory : \n \n  and the iterative process : \n \n  my training happen in round as follow : \n \n  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . \n \n  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . \n I start with a model that train well without dp to later add dp . \n  any idea why this be happen ? \n  good regard , \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n']",noise - privacy - differential - differential privacy - add - clip - add noise - use - training - model - use opacus - clipping - opacus - set - train - batch - happen - federate - local - learning,1,TRUE
105,67633859,norm clip technique TFF,"norm clip technique TFF   I m train dp federate learning model use "" dp - fedavg "" algorithm , base paper :      the paper propose two norm clip technique "" flat clipping "" "" per - layer clipping "" , perform experiment use "" per - layer clipping "" .    in case tff , attach dp - query aggregation process federate model , clip technique implement default ? way specify clip technique use ?","norm clip technique in TFF   I m train a dp federate learning model use the "" dp - fedavg "" algorithm , which be base on below paper : 
 
  the paper propose two norm clip technique "" flat clipping "" and "" per - layer clipping "" , then perform the experiment use "" per - layer clipping "" . 
  in case of TFF , when attach a dp - query and an aggregation process to the federate model , which clip technique be implement by default ? be there a way to specify the clip technique use ? 
","norm clip technique in TFF   I m train a dp federate learning model use the "" dp - fedavg "" algorithm , which be base on below paper : 
 
  the paper propose two norm clip technique "" flat clipping "" and "" per - layer clipping "" , then perform the experiment use "" per - layer clipping "" . 
  in case of TFF , when attach a dp - query and an aggregation process to the federate model , which clip technique be implement by default ? be there a way to specify the clip technique use ? 
",7,7_noise_privacy_differential_differential privacy,"['noise', 'privacy', 'differential', 'differential privacy', 'add', 'clip', 'add noise', 'use', 'training', 'model', 'use opacus', 'clipping', 'opacus', 'set', 'train', 'batch', 'happen', 'federate', 'local', 'learning']","['Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . \n I be use TFF in case someone be wonder . \n  I load my datum as : \n \n  and I set Q as sample ratio \n \n  give this I define my dp parameter : \n \n  Noise = 0.5 \n  q = 0.015 \n  n_clients_per_round = int(q*len(train_data.client_id ) ) \n \n  I define my aggregation factory : \n \n  and the iterative process : \n \n  my training happen in round as follow : \n \n  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . \n \n  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . \n I start with a model that train well without dp to later add dp . \n  any idea why this be happen ? \n  good regard , \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n']",noise - privacy - differential - differential privacy - add - clip - add noise - use - training - model - use opacus - clipping - opacus - set - train - batch - happen - federate - local - learning,0.605812093,FALSE
124,75874823,Federated learn Differential Privacy - bad test performance,"Federated learn Differential Privacy - bad test performance   I ve play time FL + dp thesis . use TFF case someone wonder .    I load datum as :      and set q sampling ratio      give define dp parameter :      Noise = 0.5    q = 0.015    n_clients_per_round = int(q*len(train_data.client_id ) )      I define aggregation factory :      and iterative process :      my training happen round follow :      the main issue training metric look good model learn slow steady rate   the test metric horrendous . look like model overfitte use dp ( know regulariser ) . absolutely confused .     I ve try several change noise learn structure modify internal round training batch size . start model train well without dp later add dp .    any idea happen ?    good regard ,","Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . 
 I be use TFF in case someone be wonder . 
  I load my datum as : 
 
  and I set Q as sample ratio 
 
  give this I define my dp parameter : 
 
  Noise = 0.5 
  q = 0.015 
  n_clients_per_round = int(q*len(train_data.client_id ) ) 
 
  I define my aggregation factory : 
 
  and the iterative process : 
 
  my training happen in round as follow : 
 
  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . 
 
  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . 
 I start with a model that train well without dp to later add dp . 
  any idea why this be happen ? 
  good regard , 
","Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . 
 I be use TFF in case someone be wonder . 
  I load my datum as : 
 
  and I set Q as sample ratio 
 
  give this I define my dp parameter : 
 
  Noise = 0.5 
  q = 0.015 
  n_clients_per_round = int(q*len(train_data.client_id ) ) 
 
  I define my aggregation factory : 
 
  and the iterative process : 
 
  my training happen in round as follow : 
 
  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . 
 
  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . 
 I start with a model that train well without dp to later add dp . 
  any idea why this be happen ? 
  good regard , 
",7,7_noise_privacy_differential_differential privacy,"['noise', 'privacy', 'differential', 'differential privacy', 'add', 'clip', 'add noise', 'use', 'training', 'model', 'use opacus', 'clipping', 'opacus', 'set', 'train', 'batch', 'happen', 'federate', 'local', 'learning']","['Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . \n I be use TFF in case someone be wonder . \n  I load my datum as : \n \n  and I set Q as sample ratio \n \n  give this I define my dp parameter : \n \n  Noise = 0.5 \n  q = 0.015 \n  n_clients_per_round = int(q*len(train_data.client_id ) ) \n \n  I define my aggregation factory : \n \n  and the iterative process : \n \n  my training happen in round as follow : \n \n  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . \n \n  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . \n I start with a model that train well without dp to later add dp . \n  any idea why this be happen ? \n  good regard , \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n']",noise - privacy - differential - differential privacy - add - clip - add noise - use - training - model - use opacus - clipping - opacus - set - train - batch - happen - federate - local - learning,1,TRUE
169,69281046,change clip noise parameter differentially private training Tensorflow Federated,"change clip noise parameter differentially private training Tensorflow Federated   I m use Tensorflow Federated ( TFF ) train differential privacy . currently create Tensorflow Privacy NormalizedQuery pass tff differentiallyprivatefactory create AggregationProcess :      after broadcast server state client run client update function use AggregationProcess like this :      this work great , however want experiment change l2_norm_clip stddev several time training ( make clip big small various training round ) seem set parameter create AggregationProcess .    be possible change parameter train somehow ?","how to change clip and noise parameter during differentially private training with Tensorflow Federated   I m use Tensorflow Federated ( TFF ) to train with differential privacy . currently I be create a Tensorflow Privacy NormalizedQuery and then pass it into a tff differentiallyprivatefactory to create an AggregationProcess : 
 
  after broadcast the server state to client I run a client update function and then use the AggregationProcess like this : 
 
  this work great , however I want to experiment with change the l2_norm_clip and stddev several time during training ( make clip big and small at various training round ) but it seem I can only set these parameter when I create the AggregationProcess . 
  be be possible to change these parameter during training somehow ? 
","how to change clip and noise parameter during differentially private training with Tensorflow Federated   I m use Tensorflow Federated ( TFF ) to train with differential privacy . currently I be create a Tensorflow Privacy NormalizedQuery and then pass it into a tff differentiallyprivatefactory to create an AggregationProcess : 
 
  after broadcast the server state to client I run a client update function and then use the AggregationProcess like this : 
 
  this work great , however I want to experiment with change the l2_norm_clip and stddev several time during training ( make clip big and small at various training round ) but it seem I can only set these parameter when I create the AggregationProcess . 
  be be possible to change these parameter during training somehow ? 
",7,7_noise_privacy_differential_differential privacy,"['noise', 'privacy', 'differential', 'differential privacy', 'add', 'clip', 'add noise', 'use', 'training', 'model', 'use opacus', 'clipping', 'opacus', 'set', 'train', 'batch', 'happen', 'federate', 'local', 'learning']","['Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . \n I be use TFF in case someone be wonder . \n  I load my datum as : \n \n  and I set Q as sample ratio \n \n  give this I define my dp parameter : \n \n  Noise = 0.5 \n  q = 0.015 \n  n_clients_per_round = int(q*len(train_data.client_id ) ) \n \n  I define my aggregation factory : \n \n  and the iterative process : \n \n  my training happen in round as follow : \n \n  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . \n \n  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . \n I start with a model that train well without dp to later add dp . \n  any idea why this be happen ? \n  good regard , \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n']",noise - privacy - differential - differential privacy - add - clip - add noise - use - training - model - use opacus - clipping - opacus - set - train - batch - happen - federate - local - learning,0.996426126,FALSE
209,70376178,implement differential privacy federate learn,"implement differential privacy federate learn   I m beginner federate learning . try add gaussian noise gradient client_updata . anyone attempt , please teach do . thank advance .  ","how implement differential privacy in federate learn   I m beginner in federated learning . 
 I try to add gaussian noise to gradient in client_updata . 
 if anyone attempt to do , please teach I how to do . 
 thank you in advance . 
 
","how implement differential privacy in federate learn   I m beginner in federated learning . 
 I try to add gaussian noise to gradient in client_updata . 
 if anyone attempt to do , please teach I how to do . 
 thank you in advance . 
 
",7,7_noise_privacy_differential_differential privacy,"['noise', 'privacy', 'differential', 'differential privacy', 'add', 'clip', 'add noise', 'use', 'training', 'model', 'use opacus', 'clipping', 'opacus', 'set', 'train', 'batch', 'happen', 'federate', 'local', 'learning']","['Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . \n I be use TFF in case someone be wonder . \n  I load my datum as : \n \n  and I set Q as sample ratio \n \n  give this I define my dp parameter : \n \n  Noise = 0.5 \n  q = 0.015 \n  n_clients_per_round = int(q*len(train_data.client_id ) ) \n \n  I define my aggregation factory : \n \n  and the iterative process : \n \n  my training happen in round as follow : \n \n  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . \n \n  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . \n I start with a model that train well without dp to later add dp . \n  any idea why this be happen ? \n  good regard , \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n']",noise - privacy - differential - differential privacy - add - clip - add noise - use - training - model - use opacus - clipping - opacus - set - train - batch - happen - federate - local - learning,1,FALSE
222,70398702,client level differential privacy Tensorflow Federated ( local dp ),"client level differential privacy Tensorflow Federated ( local dp )   I want implement local dp model use TFF , be , client train differentially private model send noisy gradient server , server aggregate distribute standard FL fashion . try change client optimizer keras dp optimizer , do not work . suggestion appreciate .","client level differential privacy in Tensorflow Federated ( local dp )   I want to implement local dp model use TFF , that is , each client train its own differentially private model and send noisy gradient to the server , and the server just aggregate and distribute in a standard FL fashion . I try change the client optimizer to keras dp optimizer , but that do not work . any suggestion be appreciate . 
","client level differential privacy in Tensorflow Federated ( local dp )   I want to implement local dp model use TFF , that is , each client train its own differentially private model and send noisy gradient to the server , and the server just aggregate and distribute in a standard FL fashion . I try change the client optimizer to keras dp optimizer , but that do not work . any suggestion be appreciate . 
",7,7_noise_privacy_differential_differential privacy,"['noise', 'privacy', 'differential', 'differential privacy', 'add', 'clip', 'add noise', 'use', 'training', 'model', 'use opacus', 'clipping', 'opacus', 'set', 'train', 'batch', 'happen', 'federate', 'local', 'learning']","['Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . \n I be use TFF in case someone be wonder . \n  I load my datum as : \n \n  and I set Q as sample ratio \n \n  give this I define my dp parameter : \n \n  Noise = 0.5 \n  q = 0.015 \n  n_clients_per_round = int(q*len(train_data.client_id ) ) \n \n  I define my aggregation factory : \n \n  and the iterative process : \n \n  my training happen in round as follow : \n \n  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . \n \n  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . \n I start with a model that train well without dp to later add dp . \n  any idea why this be happen ? \n  good regard , \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n']",noise - privacy - differential - differential privacy - add - clip - add noise - use - training - model - use opacus - clipping - opacus - set - train - batch - happen - federate - local - learning,1,FALSE
290,62311087,privacy Secrity training model use federate learning,privacy Secrity training model use federate learning   do federate learning provide privacy security model train ?,"privacy Secrity for training model when use federate learning   do federate learning provide privacy security for the model be train ?  
","privacy Secrity for training model when use federate learning   do federate learning provide privacy security for the model be train ?  
",7,7_noise_privacy_differential_differential privacy,"['noise', 'privacy', 'differential', 'differential privacy', 'add', 'clip', 'add noise', 'use', 'training', 'model', 'use opacus', 'clipping', 'opacus', 'set', 'train', 'batch', 'happen', 'federate', 'local', 'learning']","['Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . \n I be use TFF in case someone be wonder . \n  I load my datum as : \n \n  and I set Q as sample ratio \n \n  give this I define my dp parameter : \n \n  Noise = 0.5 \n  q = 0.015 \n  n_clients_per_round = int(q*len(train_data.client_id ) ) \n \n  I define my aggregation factory : \n \n  and the iterative process : \n \n  my training happen in round as follow : \n \n  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . \n \n  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . \n I start with a model that train well without dp to later add dp . \n  any idea why this be happen ? \n  good regard , \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n']",noise - privacy - differential - differential privacy - add - clip - add noise - use - training - model - use opacus - clipping - opacus - set - train - batch - happen - federate - local - learning,0.696562713,FALSE
307,72241747,add noise ( differential privacy ) client weight federal learning ?,"add noise ( differential privacy ) client weight federal learning ?   I want add noise gradient client side . modify     , work .      the error be      I add noise server side use   , add noise client side ?","how to add noise ( differential privacy ) to client weight in federal learning ?   I want to add noise to the gradient on the client side . I modify    to   , but it do not work . 
 
  the error be 
 
  I can add noise on the server side use the   , but how to add noise on the client side ? 
","how to add noise ( differential privacy ) to client weight in federal learning ?   I want to add noise to the gradient on the client side . I modify    to   , but it do not work . 
 
  the error be 
 
  I can add noise on the server side use the   , but how to add noise on the client side ? 
",7,7_noise_privacy_differential_differential privacy,"['noise', 'privacy', 'differential', 'differential privacy', 'add', 'clip', 'add noise', 'use', 'training', 'model', 'use opacus', 'clipping', 'opacus', 'set', 'train', 'batch', 'happen', 'federate', 'local', 'learning']","['Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . \n I be use TFF in case someone be wonder . \n  I load my datum as : \n \n  and I set Q as sample ratio \n \n  give this I define my dp parameter : \n \n  Noise = 0.5 \n  q = 0.015 \n  n_clients_per_round = int(q*len(train_data.client_id ) ) \n \n  I define my aggregation factory : \n \n  and the iterative process : \n \n  my training happen in round as follow : \n \n  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . \n \n  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . \n I start with a model that train well without dp to later add dp . \n  any idea why this be happen ? \n  good regard , \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n']",noise - privacy - differential - differential privacy - add - clip - add noise - use - training - model - use opacus - clipping - opacus - set - train - batch - happen - federate - local - learning,1,FALSE
313,62544709,additive noise need calibrate sensitivity differential privacy ?,"additive noise need calibrate sensitivity differential privacy ?   as beginner differential privacy , would like variance noise mechanism need calibrate sensitivity ? purpose that ? happen calibrate add random variance ?    Example scenario   laplacian noise , scale parameter calibrate ?","why additive noise need to be calibrate with sensitivity in differential privacy ?   as a beginner to differential privacy , I would like to why the variance for noise mechanism need to be calibrate with sensitivity ? what be the purpose of that ? what happen if we do not calibrate it and add a random variance ? 
  Example scenario   in laplacian noise , why scale parameter be calibrate ? 
","why additive noise need to be calibrate with sensitivity in differential privacy ?   as a beginner to differential privacy , I would like to why the variance for noise mechanism need to be calibrate with sensitivity ? what be the purpose of that ? what happen if we do not calibrate it and add a random variance ? 
  Example scenario   in laplacian noise , why scale parameter be calibrate ? 
",7,7_noise_privacy_differential_differential privacy,"['noise', 'privacy', 'differential', 'differential privacy', 'add', 'clip', 'add noise', 'use', 'training', 'model', 'use opacus', 'clipping', 'opacus', 'set', 'train', 'batch', 'happen', 'federate', 'local', 'learning']","['Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . \n I be use TFF in case someone be wonder . \n  I load my datum as : \n \n  and I set Q as sample ratio \n \n  give this I define my dp parameter : \n \n  Noise = 0.5 \n  q = 0.015 \n  n_clients_per_round = int(q*len(train_data.client_id ) ) \n \n  I define my aggregation factory : \n \n  and the iterative process : \n \n  my training happen in round as follow : \n \n  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . \n \n  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . \n I start with a model that train well without dp to later add dp . \n  any idea why this be happen ? \n  good regard , \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n']",noise - privacy - differential - differential privacy - add - clip - add noise - use - training - model - use opacus - clipping - opacus - set - train - batch - happen - federate - local - learning,1,FALSE
361,73112324,create tensorflow - federate system local differential privacy ?,"create tensorflow - federate system local differential privacy ?   I create centralized differential privacy system accord official manual Tensorflow - federate . however , research need local differential privacy system base Tensorflow - federate . anyone know it ?","how to create a tensorflow - federate system by local differential privacy ?   I have create a centralized differential privacy system accord to the official manual of tensorflow - federate . however , my research need a local differential privacy system base on Tensorflow - federate . do anyone know how to do it ? 
","how to create a tensorflow - federate system by local differential privacy ?   I have create a centralized differential privacy system accord to the official manual of tensorflow - federate . however , my research need a local differential privacy system base on Tensorflow - federate . do anyone know how to do it ? 
",7,7_noise_privacy_differential_differential privacy,"['noise', 'privacy', 'differential', 'differential privacy', 'add', 'clip', 'add noise', 'use', 'training', 'model', 'use opacus', 'clipping', 'opacus', 'set', 'train', 'batch', 'happen', 'federate', 'local', 'learning']","['Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . \n I be use TFF in case someone be wonder . \n  I load my datum as : \n \n  and I set Q as sample ratio \n \n  give this I define my dp parameter : \n \n  Noise = 0.5 \n  q = 0.015 \n  n_clients_per_round = int(q*len(train_data.client_id ) ) \n \n  I define my aggregation factory : \n \n  and the iterative process : \n \n  my training happen in round as follow : \n \n  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . \n \n  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . \n I start with a model that train well without dp to later add dp . \n  any idea why this be happen ? \n  good regard , \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n']",noise - privacy - differential - differential privacy - add - clip - add noise - use - training - model - use opacus - clipping - opacus - set - train - batch - happen - federate - local - learning,0.973456393,FALSE
368,73199007,noisemultipli mean tensorflow - federate tutorial ?,"noisemultipli mean tensorflow - federate tutorial ?   I find term    follow part tensorflow - federate tutorial .      I read paper differential privacy adaptive clipping . guess    noise input system . however , find    scalar set . actually , different noise put correspond weight_variable , confuse that .","what do noisemultipli mean in tensorflow - federate tutorial ?   I find the term    in the follow part of tensorflow - federate tutorial . 
 
  I have read the paper about differential privacy with adaptive clipping . I guess    be the noise we input to the system . however , I find the    be a scalar we set . actually , the different noise should be put into the corresponding weight_variable , so I be so confused about that . 
","what do noisemultipli mean in tensorflow - federate tutorial ?   I find the term    in the follow part of tensorflow - federate tutorial . 
 
  I have read the paper about differential privacy with adaptive clipping . I guess    be the noise we input to the system . however , I find the    be a scalar we set . actually , the different noise should be put into the corresponding weight_variable , so I be so confused about that . 
",7,7_noise_privacy_differential_differential privacy,"['noise', 'privacy', 'differential', 'differential privacy', 'add', 'clip', 'add noise', 'use', 'training', 'model', 'use opacus', 'clipping', 'opacus', 'set', 'train', 'batch', 'happen', 'federate', 'local', 'learning']","['Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . \n I be use TFF in case someone be wonder . \n  I load my datum as : \n \n  and I set Q as sample ratio \n \n  give this I define my dp parameter : \n \n  Noise = 0.5 \n  q = 0.015 \n  n_clients_per_round = int(q*len(train_data.client_id ) ) \n \n  I define my aggregation factory : \n \n  and the iterative process : \n \n  my training happen in round as follow : \n \n  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . \n \n  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . \n I start with a model that train well without dp to later add dp . \n  any idea why this be happen ? \n  good regard , \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n']",noise - privacy - differential - differential privacy - add - clip - add noise - use - training - model - use opacus - clipping - opacus - set - train - batch - happen - federate - local - learning,1,FALSE
377,73502727,noise addition weight use Opacus Federated Learning set,"noise addition weight use Opacus Federated Learning set   I plan use Opacus implement differential privacy federate learning model basic doubt would love clear that .    so far understanding go , use Opacus , use optimizer like DPSGD add differential noise batch clientâ€ ™ s dataset â€œlocal trainingâ€. federate learning , train client model â€œlocal epochsâ€ send weight central server aggregation , add differential noise send model weight .    so question be , use DPSGD add noise every single batch every single client dataset local training could add noise local weight send out ? let local training epoch happen simply add noise outbound weight time departure ? miss ?","noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . 
  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€. and in federated learning , we train client model for a few â€œlocal epochsâ€ before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . 
  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? 
","noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . 
  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€. and in federated learning , we train client model for a few â€œlocal epochsâ€ before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . 
  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? 
",7,7_noise_privacy_differential_differential privacy,"['noise', 'privacy', 'differential', 'differential privacy', 'add', 'clip', 'add noise', 'use', 'training', 'model', 'use opacus', 'clipping', 'opacus', 'set', 'train', 'batch', 'happen', 'federate', 'local', 'learning']","['Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . \n I be use TFF in case someone be wonder . \n  I load my datum as : \n \n  and I set Q as sample ratio \n \n  give this I define my dp parameter : \n \n  Noise = 0.5 \n  q = 0.015 \n  n_clients_per_round = int(q*len(train_data.client_id ) ) \n \n  I define my aggregation factory : \n \n  and the iterative process : \n \n  my training happen in round as follow : \n \n  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . \n \n  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . \n I start with a model that train well without dp to later add dp . \n  any idea why this be happen ? \n  good regard , \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n']",noise - privacy - differential - differential privacy - add - clip - add noise - use - training - model - use opacus - clipping - opacus - set - train - batch - happen - federate - local - learning,1,TRUE
386,64046242,apply Differential Privacy TensorFlow Federated,"apply Differential Privacy TensorFlow Federated   I try use Tensorflow Privacy TFF follow two example provide   dataset . make sure sample target format correctly everything work add dp process clip noise . unfortunately , execution dp enable model diverge instead converge , train validation loss increase round .      I try different combination clip noise_multipli without achieve result .. example :      any idea could problem ? noise_multipli : false everything work properly .. definition dp_query averaging process basically use example :      thank you !","apply Differential Privacy in TensorFlow Federated   I be try to use Tensorflow Privacy with TFF follow the two example provide in   with my own dataset . 
 I make sure that sample and target be format correctly and everything work before add the dp process with clipping and noise . 
 unfortunately , in any execution with dp enable the model diverge instead of converge , with both train and validation loss increase at each round . 
 
  I have try with different combination of clip and noise_multipli but without achieve any result .. 
 here be an example : 
 
  any idea on what could be the problem ? with noise_multipli : false everything be work properly .. 
 the definition of the dp_query and the averaging process be basically the same use in the example : 
 
  thank you ! 
","apply Differential Privacy in TensorFlow Federated   I be try to use Tensorflow Privacy with TFF follow the two example provide in   with my own dataset . 
 I make sure that sample and target be format correctly and everything work before add the dp process with clipping and noise . 
 unfortunately , in any execution with dp enable the model diverge instead of converge , with both train and validation loss increase at each round . 
 
  I have try with different combination of clip and noise_multipli but without achieve any result .. 
 here be an example : 
 
  any idea on what could be the problem ? with noise_multipli : false everything be work properly .. 
 the definition of the dp_query and the averaging process be basically the same use in the example : 
 
  thank you ! 
",7,7_noise_privacy_differential_differential privacy,"['noise', 'privacy', 'differential', 'differential privacy', 'add', 'clip', 'add noise', 'use', 'training', 'model', 'use opacus', 'clipping', 'opacus', 'set', 'train', 'batch', 'happen', 'federate', 'local', 'learning']","['Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . \n I be use TFF in case someone be wonder . \n  I load my datum as : \n \n  and I set Q as sample ratio \n \n  give this I define my dp parameter : \n \n  Noise = 0.5 \n  q = 0.015 \n  n_clients_per_round = int(q*len(train_data.client_id ) ) \n \n  I define my aggregation factory : \n \n  and the iterative process : \n \n  my training happen in round as follow : \n \n  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . \n \n  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . \n I start with a model that train well without dp to later add dp . \n  any idea why this be happen ? \n  good regard , \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n']",noise - privacy - differential - differential privacy - add - clip - add noise - use - training - model - use opacus - clipping - opacus - set - train - batch - happen - federate - local - learning,1,FALSE
432,75874823,Federated learn Differential Privacy - bad test performance,"Federated learn Differential Privacy - bad test performance   I ve play time FL + dp thesis . use TFF case someone wonder .    I load datum as :      and set q sampling ratio      give define dp parameter :      Noise = 0.5    q = 0.015    n_clients_per_round = int(q*len(train_data.client_id ) )      I define aggregation factory :      and iterative process :      my training happen round follow :      the main issue training metric look good model learn slow steady rate   the test metric horrendous . look like model overfitte use dp ( know regulariser ) . absolutely confused .     I ve try several change noise learn structure modify internal round training batch size . start model train well without dp later add dp .    any idea happen ?    good regard ,","Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . 
 I be use TFF in case someone be wonder . 
  I load my datum as : 
 
  and I set Q as sample ratio 
 
  give this I define my dp parameter : 
 
  Noise = 0.5 
  q = 0.015 
  n_clients_per_round = int(q*len(train_data.client_id ) ) 
 
  I define my aggregation factory : 
 
  and the iterative process : 
 
  my training happen in round as follow : 
 
  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . 
 
  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . 
 I start with a model that train well without dp to later add dp . 
  any idea why this be happen ? 
  good regard , 
","Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . 
 I be use TFF in case someone be wonder . 
  I load my datum as : 
 
  and I set Q as sample ratio 
 
  give this I define my dp parameter : 
 
  Noise = 0.5 
  q = 0.015 
  n_clients_per_round = int(q*len(train_data.client_id ) ) 
 
  I define my aggregation factory : 
 
  and the iterative process : 
 
  my training happen in round as follow : 
 
  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . 
 
  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . 
 I start with a model that train well without dp to later add dp . 
  any idea why this be happen ? 
  good regard , 
",7,7_noise_privacy_differential_differential privacy,"['noise', 'privacy', 'differential', 'differential privacy', 'add', 'clip', 'add noise', 'use', 'training', 'model', 'use opacus', 'clipping', 'opacus', 'set', 'train', 'batch', 'happen', 'federate', 'local', 'learning']","['Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . \n I be use TFF in case someone be wonder . \n  I load my datum as : \n \n  and I set Q as sample ratio \n \n  give this I define my dp parameter : \n \n  Noise = 0.5 \n  q = 0.015 \n  n_clients_per_round = int(q*len(train_data.client_id ) ) \n \n  I define my aggregation factory : \n \n  and the iterative process : \n \n  my training happen in round as follow : \n \n  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . \n \n  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . \n I start with a model that train well without dp to later add dp . \n  any idea why this be happen ? \n  good regard , \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n']",noise - privacy - differential - differential privacy - add - clip - add noise - use - training - model - use opacus - clipping - opacus - set - train - batch - happen - federate - local - learning,0.952297672,TRUE
446,65943395,track privacy guarantee federate learn process dp - query,"track privacy guarantee federate learn process dp - query   I m bit new TFF , check github follow EMNIST example train differentially private federate model use    algorithm . mainly do attach      train federate model .    I question please :    1 . give attach    aggregation process would result participant - level central - dp , would track privacy guarantee ( eps , delta ) training ?    below code snippet differentially private federate model set 100 participant ,      set 100      I come across several method compute epsilon delta TF - Privacy , seem meant track privacy guarantee traditional    algorithm expect receive parameter   ,       thank lot advance","track privacy guarantee in a federate learning process with dp - query   I m a bit new to TFF , I have check github and follow the emnist example to train a differentially private federate model use    algorithm . mainly this be do by attach a    to the    then train the federate model . 
  I have a question please : 
  1 . give that attach a    to the aggregation process would result in a participant - level central - dp ,   how would I track the privacy guarantee ( eps , delta ) during training ? 
  below be a code snippet where a differentially private federate model be set up with 100 participant , that be why both    and    be set to 100 
 
  I come across several method to compute epsilon and delta in TF - Privacy , but it seem they be mean to track privacy guarantee of the traditional    algorithm and expect to receive parameter such as   ,     and  
  thank a lot in advance 
","track privacy guarantee in a federate learning process with dp - query   I m a bit new to TFF , I have check github and follow the emnist example to train a differentially private federate model use    algorithm . mainly this be do by attach a    to the    then train the federate model . 
  I have a question please : 
  1 . give that attach a    to the aggregation process would result in a participant - level central - dp ,   how would I track the privacy guarantee ( eps , delta ) during training ? 
  below be a code snippet where a differentially private federate model be set up with 100 participant , that be why both    and    be set to 100 
 
  I come across several method to compute epsilon and delta in TF - Privacy , but it seem they be mean to track privacy guarantee of the traditional    algorithm and expect to receive parameter such as   ,     and  
  thank a lot in advance 
",7,7_noise_privacy_differential_differential privacy,"['noise', 'privacy', 'differential', 'differential privacy', 'add', 'clip', 'add noise', 'use', 'training', 'model', 'use opacus', 'clipping', 'opacus', 'set', 'train', 'batch', 'happen', 'federate', 'local', 'learning']","['Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . \n I be use TFF in case someone be wonder . \n  I load my datum as : \n \n  and I set Q as sample ratio \n \n  give this I define my dp parameter : \n \n  Noise = 0.5 \n  q = 0.015 \n  n_clients_per_round = int(q*len(train_data.client_id ) ) \n \n  I define my aggregation factory : \n \n  and the iterative process : \n \n  my training happen in round as follow : \n \n  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . \n \n  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . \n I start with a model that train well without dp to later add dp . \n  any idea why this be happen ? \n  good regard , \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n']",noise - privacy - differential - differential privacy - add - clip - add noise - use - training - model - use opacus - clipping - opacus - set - train - batch - happen - federate - local - learning,1,FALSE
464,66413948,per example clip TensorFlow Federated dp - fedavg,"per example clip TensorFlow Federated dp - FedAvg   I m try train differentially private federate model use emnist dataset , attach     . moreover , I m resemble    algorithm use    client server optimizer server learning rate set 1 .    the query be :      what type clip query perform ,   batch clip ,   per example clip ?    in   , option use per example clip set      default   .    how something similar here , switch batch clip per example clip Federated setting ?","per example clip in TensorFlow Federated with dp - FedAvg   I m try to train a differentially private federate model use emnist dataset , I have attach the    to the   . moreover , I m resemble the    algorithm by use    as both client and server optimizer with server learning rate set to 1 . 
  the query be : 
 
  what type of clipping do this query perform , be it   batch clip , or   per example clip ? 
  in   , the option to use the per example clip be to set the    to    so it default to the   . 
  how can I do something similar here , and switch between the batch clip and per example clip in Federated setting ? 
","per example clip in TensorFlow Federated with dp - FedAvg   I m try to train a differentially private federate model use emnist dataset , I have attach the    to the   . moreover , I m resemble the    algorithm by use    as both client and server optimizer with server learning rate set to 1 . 
  the query be : 
 
  what type of clipping do this query perform , be it   batch clip , or   per example clip ? 
  in   , the option to use the per example clip be to set the    to    so it default to the   . 
  how can I do something similar here , and switch between the batch clip and per example clip in Federated setting ? 
",7,7_noise_privacy_differential_differential privacy,"['noise', 'privacy', 'differential', 'differential privacy', 'add', 'clip', 'add noise', 'use', 'training', 'model', 'use opacus', 'clipping', 'opacus', 'set', 'train', 'batch', 'happen', 'federate', 'local', 'learning']","['Federated learning with Differential Privacy - bad test performance   I ve be play for some time with FL + dp for my thesis . \n I be use TFF in case someone be wonder . \n  I load my datum as : \n \n  and I set Q as sample ratio \n \n  give this I define my dp parameter : \n \n  Noise = 0.5 \n  q = 0.015 \n  n_clients_per_round = int(q*len(train_data.client_id ) ) \n \n  I define my aggregation factory : \n \n  and the iterative process : \n \n  my training happen in round as follow : \n \n  the main issue here be that the training metric look good and the model learn at a slow but steady rate but   the test metric be horrendous . it look like the model be overfitte while use dp ( know to be a regulariser ) . I be absolutely confused . \n \n  I ve try several change in the noise and learn structure so as modify the internal round of training and the batch size . \n I start with a model that train well without dp to later add dp . \n  any idea why this be happen ? \n  good regard , \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n', 'noise addition to weight use Opacus in a Federated Learning set   I be plan to use Opacus to implement differential privacy in my federate learning model but I have a very basic doubt that I would love to have clear before that . \n  so as far as my understanding go , use Opacus , we use an optimizer like DPSGD that add differential noise to each batch of each clientâ€ ™ s dataset while they be in â€œlocal trainingâ€\x9d. and in federated learning , we train client model for a few â€œlocal epochsâ€\x9d before send their weight out to a central server for aggregation , and we add differential noise before send out the model weight . \n  so my question be , why do we use DPSGD to add noise to every single batch of every single client dataset during local training when we could just add noise to the local weight before they be send out ? why do we not let the local training epoch happen as be and simply add noise to the outbound weight at the time of departure ? what be I miss ? \n']",noise - privacy - differential - differential privacy - add - clip - add noise - use - training - model - use opacus - clipping - opacus - set - train - batch - happen - federate - local - learning,0.713446137,FALSE
