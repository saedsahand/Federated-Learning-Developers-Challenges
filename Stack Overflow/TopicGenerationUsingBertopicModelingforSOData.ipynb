{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b52929e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertopic in /home/kha060/.local/lib/python3.10/site-packages (0.15.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/kha060/.local/lib/python3.10/site-packages (from bertopic) (1.24.4)\n",
      "Requirement already satisfied: hdbscan>=0.8.29 in /home/kha060/.local/lib/python3.10/site-packages (from bertopic) (0.8.33)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in /home/kha060/.local/lib/python3.10/site-packages (from bertopic) (0.5.3)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /home/kha060/.local/lib/python3.10/site-packages (from bertopic) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /home/kha060/.local/lib/python3.10/site-packages (from bertopic) (1.3.0)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /home/kha060/.local/lib/python3.10/site-packages (from bertopic) (4.66.1)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from bertopic) (2.7.0)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from bertopic) (5.24.1)\n",
      "Requirement already satisfied: cython<3,>=0.27 in /home/kha060/.local/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (0.29.36)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/kha060/.local/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/kha060/.local/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/kha060/.local/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/kha060/.local/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2023.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/kha060/.local/lib/python3.10/site-packages (from plotly>=4.7.0->bertopic) (8.2.3)\n",
      "Requirement already satisfied: packaging in /home/kha060/.local/lib/python3.10/site-packages (from plotly>=4.7.0->bertopic) (23.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/kha060/.local/lib/python3.10/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.2.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.40.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/kha060/.local/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (2.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.23.0)\n",
      "Requirement already satisfied: Pillow in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (10.2.0)\n",
      "Requirement already satisfied: numba>=0.49 in /home/kha060/.local/lib/python3.10/site-packages (from umap-learn>=0.5.0->bertopic) (0.57.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/kha060/.local/lib/python3.10/site-packages (from umap-learn>=0.5.0->bertopic) (0.5.10)\n",
      "Requirement already satisfied: filelock in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/kha060/.local/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2023.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (4.12.2)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /home/kha060/.local/lib/python3.10/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.40.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: sympy in /home/kha060/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.12)\n",
      "Requirement already satisfied: networkx in /home/kha060/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/kha060/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/kha060/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/kha060/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/kha060/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/kha060/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/kha060/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/kha060/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/kha060/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/kha060/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/kha060/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/kha060/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/kha060/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/kha060/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (68.2.2)\n",
      "Requirement already satisfied: wheel in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (0.41.2)\n",
      "Requirement already satisfied: cmake in /home/kha060/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.27.2)\n",
      "Requirement already satisfied: lit in /home/kha060/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (16.0.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kha060/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.4.1->bertopic) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.4.1->bertopic) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.4.1->bertopic) (0.4.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/kha060/.local/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/kha060/.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/kha060/.local/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5f01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "031e9431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>OriginalText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32333312</td>\n",
       "      <td>extract chunk BIO chunk sentence ? - python</td>\n",
       "      <td>extract chunk BIO chunk sentence ? - python   ...</td>\n",
       "      <td>How to extract chunks from BIO chunked sentenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33695244</td>\n",
       "      <td>use Completion Suggester match ngram query</td>\n",
       "      <td>use Completion Suggester match ngram query   I...</td>\n",
       "      <td>Use Completion Suggester to match against all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33941091</td>\n",
       "      <td>chunk document test plagiarism</td>\n",
       "      <td>chunk document test plagiarism   I build plagi...</td>\n",
       "      <td>Chunking documents to test for plagiarism  I a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34090734</td>\n",
       "      <td>use nltk regex pattern extract specific phrase...</td>\n",
       "      <td>use nltk regex pattern extract specific phrase...</td>\n",
       "      <td>How to use nltk regex pattern to extract a spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34318427</td>\n",
       "      <td>itextsharp : word break split textchunk word</td>\n",
       "      <td>itextsharp : word break split textchunk word  ...</td>\n",
       "      <td>itextsharp: words are broken when splitting te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8588</th>\n",
       "      <td>78981951</td>\n",
       "      <td>set random seed Chroma DB ?</td>\n",
       "      <td>set random seed Chroma DB ?   I m experiment d...</td>\n",
       "      <td>How do I set the random seed for Chroma DB?  I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8589</th>\n",
       "      <td>78982153</td>\n",
       "      <td>send parameter directly LLM langchain</td>\n",
       "      <td>send parameter directly LLM langchain   curren...</td>\n",
       "      <td>How to send a parameter directly to LLM in lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8590</th>\n",
       "      <td>78984423</td>\n",
       "      <td>Azure Document Intelligence Custom Classificat...</td>\n",
       "      <td>Azure Document Intelligence Custom Classificat...</td>\n",
       "      <td>Azure Document Intelligence Custom Classificat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8591</th>\n",
       "      <td>78984512</td>\n",
       "      <td>protect Routes Edge - Runtime t3 Stack / Verce...</td>\n",
       "      <td>protect Routes Edge - Runtime t3 Stack / Verce...</td>\n",
       "      <td>Protecting Routes in an Edge-Runtime with T3 S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8592</th>\n",
       "      <td>78985031</td>\n",
       "      <td>yolov5 Class Imbalance Overfitting issue</td>\n",
       "      <td>yolov5 Class Imbalance Overfitting Issues   I ...</td>\n",
       "      <td>YOLOv5 Class Imbalance and Overfitting Issues ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8593 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id                                              Title  \\\n",
       "0     32333312        extract chunk BIO chunk sentence ? - python   \n",
       "1     33695244         use Completion Suggester match ngram query   \n",
       "2     33941091                     chunk document test plagiarism   \n",
       "3     34090734  use nltk regex pattern extract specific phrase...   \n",
       "4     34318427       itextsharp : word break split textchunk word   \n",
       "...        ...                                                ...   \n",
       "8588  78981951                        set random seed Chroma DB ?   \n",
       "8589  78982153              send parameter directly LLM langchain   \n",
       "8590  78984423  Azure Document Intelligence Custom Classificat...   \n",
       "8591  78984512  protect Routes Edge - Runtime t3 Stack / Verce...   \n",
       "8592  78985031           yolov5 Class Imbalance Overfitting issue   \n",
       "\n",
       "                                                   Text  \\\n",
       "0     extract chunk BIO chunk sentence ? - python   ...   \n",
       "1     use Completion Suggester match ngram query   I...   \n",
       "2     chunk document test plagiarism   I build plagi...   \n",
       "3     use nltk regex pattern extract specific phrase...   \n",
       "4     itextsharp : word break split textchunk word  ...   \n",
       "...                                                 ...   \n",
       "8588  set random seed Chroma DB ?   I m experiment d...   \n",
       "8589  send parameter directly LLM langchain   curren...   \n",
       "8590  Azure Document Intelligence Custom Classificat...   \n",
       "8591  protect Routes Edge - Runtime t3 Stack / Verce...   \n",
       "8592  yolov5 Class Imbalance Overfitting Issues   I ...   \n",
       "\n",
       "                                           OriginalText  \n",
       "0     How to extract chunks from BIO chunked sentenc...  \n",
       "1     Use Completion Suggester to match against all ...  \n",
       "2     Chunking documents to test for plagiarism  I a...  \n",
       "3     How to use nltk regex pattern to extract a spe...  \n",
       "4     itextsharp: words are broken when splitting te...  \n",
       "...                                                 ...  \n",
       "8588  How do I set the random seed for Chroma DB?  I...  \n",
       "8589  How to send a parameter directly to LLM in lan...  \n",
       "8590  Azure Document Intelligence Custom Classificat...  \n",
       "8591  Protecting Routes in an Edge-Runtime with T3 S...  \n",
       "8592  YOLOv5 Class Imbalance and Overfitting Issues ...  \n",
       "\n",
       "[8593 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df = pd.read_csv('CleanedLLMsPostsFromSO.csv')\n",
    "cols = ['Id', 'Title', 'Text', 'OriginalText']\n",
    "preprocessed_df = preprocessed_df[cols]\n",
    "preprocessed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2598b3d",
   "metadata": {},
   "source": [
    "# Pre-calculate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b73f4927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.0.0.dev0, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "/home/kha060/anaconda3/envs/BTI/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "## Precalculate Embedding\n",
    "embedding_model = SentenceTransformer(\"multi-qa-MiniLM-L6-dot-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ea45ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 12:26:47.143651: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-10 12:26:47.144976: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-10 12:26:47.161116: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-10 12:26:47.161134: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-10 12:26:47.161784: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-10 12:26:47.164935: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-10 12:26:47.650262: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-10-10 12:26:47.858170: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-10 12:26:47.859126: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "[nltk_data] Downloading package punkt to /home/kha060/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Lemmatize Original Text\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nlp =  spacy.load('en_core_web_sm')\n",
    "import spacy\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return ' '. join(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a643697e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       how to extract chunk from BIO chunk sentence ?...\n",
       "1       use Completion Suggester to match against all ...\n",
       "2       chunk document to test for plagiarism   I be b...\n",
       "3       how to use nltk regex pattern to extract a spe...\n",
       "4       itextsharp : word be break when split textchun...\n",
       "                              ...                        \n",
       "8588    how do I set the random seed for Chroma DB ?  ...\n",
       "8589    how to send a parameter directly to LLM in lan...\n",
       "8590    Azure Document Intelligence Custom Classificat...\n",
       "8591    Protecting Routes in an Edge - runtime with t3...\n",
       "8592    yolov5 Class Imbalance and Overfitting Issues ...\n",
       "Name: OriginalText, Length: 8593, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df['OriginalText'] = preprocessed_df['OriginalText'].apply(lemmatize_text)\n",
    "preprocessed_df['OriginalText']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2233f2e4",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e7bdc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kha060/.local/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/kha060/.local/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/kha060/.local/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/home/kha060/.local/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>UMAP(n_components=5, n_neighbors=10, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">UMAP</label><div class=\"sk-toggleable__content\"><pre>UMAP(n_components=5, n_neighbors=10, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "UMAP(n_components=5, n_neighbors=10, random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.1, metric='euclidean', random_state=42)\n",
    "umap_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21a09cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HDBSCAN(cluster_selection_method=&#x27;leaf&#x27;, min_cluster_size=150,\n",
       "        prediction_data=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HDBSCAN</label><div class=\"sk-toggleable__content\"><pre>HDBSCAN(cluster_selection_method=&#x27;leaf&#x27;, min_cluster_size=150,\n",
       "        prediction_data=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "HDBSCAN(cluster_selection_method='leaf', min_cluster_size=150,\n",
       "        prediction_data=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=150, metric='euclidean', cluster_selection_method='leaf', prediction_data=True)\n",
    "\n",
    "hdbscan_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bed13d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4836090e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa589309b9334779ab093eba33042aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/269 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Embedding Generation\n",
    "embeddings_OriginalText = embedding_model.encode(preprocessed_df['OriginalText'], show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11c6c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model = embedding_model,\n",
    "    umap_model = umap_model,\n",
    "    hdbscan_model = hdbscan_model,\n",
    "    vectorizer_model = vectorizer_model,\n",
    "    top_n_words = 20,\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa920e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-10 12:28:58,935 - BERTopic - Reduced dimensionality\n",
      "2024-10-10 12:28:59,368 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>4344</td>\n",
       "      <td>-1_use_model_try_error</td>\n",
       "      <td>[use, model, try, error, code, langchain, work...</td>\n",
       "      <td>[streamlit not take more than one prompt for c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1782</td>\n",
       "      <td>0_api_openai_use_error</td>\n",
       "      <td>[api, openai, use, error, code, response, try,...</td>\n",
       "      <td>[OpenAI API not read api key in my .env.local ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>606</td>\n",
       "      <td>1_gym_environment_openai gym_use</td>\n",
       "      <td>[gym, environment, openai gym, use, action, tr...</td>\n",
       "      <td>[error in import environment OpenAI Gym / can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>440</td>\n",
       "      <td>2_model_bert_huggingface_use</td>\n",
       "      <td>[model, bert, huggingface, use, train, transfo...</td>\n",
       "      <td>[how Fine - tune my train model ( bert ) on an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>394</td>\n",
       "      <td>3_vector_document_chromadb_use</td>\n",
       "      <td>[vector, document, chromadb, use, chroma, stor...</td>\n",
       "      <td>[how to retrieve id and metadata associate wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>361</td>\n",
       "      <td>4_llama_gpu_model_use</td>\n",
       "      <td>[llama, gpu, model, use, run, error, index, tr...</td>\n",
       "      <td>[unable for send multiple input use Llama CPP ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>258</td>\n",
       "      <td>5_whisper_audio_file_transcribe</td>\n",
       "      <td>[whisper, audio, file, transcribe, use, audio ...</td>\n",
       "      <td>[I be use Whisper to transcribe and I be get t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>212</td>\n",
       "      <td>6_langchain_error_import_try</td>\n",
       "      <td>[langchain, error, import, try, use, code, pyt...</td>\n",
       "      <td>[can not run simple intro langchain applicatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>196</td>\n",
       "      <td>7_langchain_tool_use_agent</td>\n",
       "      <td>[langchain, tool, use, agent, chain, prompt, a...</td>\n",
       "      <td>[LangChain , terminate a chain on specific too...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                              Name  \\\n",
       "0     -1   4344            -1_use_model_try_error   \n",
       "1      0   1782            0_api_openai_use_error   \n",
       "2      1    606  1_gym_environment_openai gym_use   \n",
       "3      2    440      2_model_bert_huggingface_use   \n",
       "4      3    394    3_vector_document_chromadb_use   \n",
       "5      4    361             4_llama_gpu_model_use   \n",
       "6      5    258   5_whisper_audio_file_transcribe   \n",
       "7      6    212      6_langchain_error_import_try   \n",
       "8      7    196        7_langchain_tool_use_agent   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [use, model, try, error, code, langchain, work...   \n",
       "1  [api, openai, use, error, code, response, try,...   \n",
       "2  [gym, environment, openai gym, use, action, tr...   \n",
       "3  [model, bert, huggingface, use, train, transfo...   \n",
       "4  [vector, document, chromadb, use, chroma, stor...   \n",
       "5  [llama, gpu, model, use, run, error, index, tr...   \n",
       "6  [whisper, audio, file, transcribe, use, audio ...   \n",
       "7  [langchain, error, import, try, use, code, pyt...   \n",
       "8  [langchain, tool, use, agent, chain, prompt, a...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [streamlit not take more than one prompt for c...  \n",
       "1  [OpenAI API not read api key in my .env.local ...  \n",
       "2  [error in import environment OpenAI Gym / can ...  \n",
       "3  [how Fine - tune my train model ( bert ) on an...  \n",
       "4  [how to retrieve id and metadata associate wit...  \n",
       "5  [unable for send multiple input use Llama CPP ...  \n",
       "6  [I be use Whisper to transcribe and I be get t...  \n",
       "7  [can not run simple intro langchain applicatio...  \n",
       "8  [LangChain , terminate a chain on specific too...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Topics Generation\n",
    "topics, probs = topic_model.fit_transform(preprocessed_df['OriginalText'], embeddings_OriginalText)\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f71188c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info().to_csv('/home/kha060/PhD/Developer Challenges LLM/Stack_Overflow/GeneratedTopics/LLM-topics_info-Using-SO-data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4054fe70",
   "metadata": {},
   "source": [
    "# Document Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c615c4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "      <th>Top_n_words</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Representative_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how to extract chunk from BIO chunk sentence ?...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_use_model_try_error</td>\n",
       "      <td>[use, model, try, error, code, langchain, work...</td>\n",
       "      <td>[streamlit not take more than one prompt for c...</td>\n",
       "      <td>use - model - try - error - code - langchain -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>use Completion Suggester to match against all ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_use_model_try_error</td>\n",
       "      <td>[use, model, try, error, code, langchain, work...</td>\n",
       "      <td>[streamlit not take more than one prompt for c...</td>\n",
       "      <td>use - model - try - error - code - langchain -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chunk document to test for plagiarism   I be b...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_use_model_try_error</td>\n",
       "      <td>[use, model, try, error, code, langchain, work...</td>\n",
       "      <td>[streamlit not take more than one prompt for c...</td>\n",
       "      <td>use - model - try - error - code - langchain -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how to use nltk regex pattern to extract a spe...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_use_model_try_error</td>\n",
       "      <td>[use, model, try, error, code, langchain, work...</td>\n",
       "      <td>[streamlit not take more than one prompt for c...</td>\n",
       "      <td>use - model - try - error - code - langchain -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>itextsharp : word be break when split textchun...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_use_model_try_error</td>\n",
       "      <td>[use, model, try, error, code, langchain, work...</td>\n",
       "      <td>[streamlit not take more than one prompt for c...</td>\n",
       "      <td>use - model - try - error - code - langchain -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8588</th>\n",
       "      <td>how do I set the random seed for Chroma DB ?  ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3_vector_document_chromadb_use</td>\n",
       "      <td>[vector, document, chromadb, use, chroma, stor...</td>\n",
       "      <td>[how to retrieve id and metadata associate wit...</td>\n",
       "      <td>vector - document - chromadb - use - chroma - ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8589</th>\n",
       "      <td>how to send a parameter directly to LLM in lan...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_use_model_try_error</td>\n",
       "      <td>[use, model, try, error, code, langchain, work...</td>\n",
       "      <td>[streamlit not take more than one prompt for c...</td>\n",
       "      <td>use - model - try - error - code - langchain -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8590</th>\n",
       "      <td>Azure Document Intelligence Custom Classificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_api_openai_use_error</td>\n",
       "      <td>[api, openai, use, error, code, response, try,...</td>\n",
       "      <td>[OpenAI API not read api key in my .env.local ...</td>\n",
       "      <td>api - openai - use - error - code - response -...</td>\n",
       "      <td>0.898531</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8591</th>\n",
       "      <td>Protecting Routes in an Edge - runtime with t3...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_api_openai_use_error</td>\n",
       "      <td>[api, openai, use, error, code, response, try,...</td>\n",
       "      <td>[OpenAI API not read api key in my .env.local ...</td>\n",
       "      <td>api - openai - use - error - code - response -...</td>\n",
       "      <td>0.927037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8592</th>\n",
       "      <td>yolov5 Class Imbalance and Overfitting Issues ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_use_model_try_error</td>\n",
       "      <td>[use, model, try, error, code, langchain, work...</td>\n",
       "      <td>[streamlit not take more than one prompt for c...</td>\n",
       "      <td>use - model - try - error - code - langchain -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8593 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Document  Topic  \\\n",
       "0     how to extract chunk from BIO chunk sentence ?...     -1   \n",
       "1     use Completion Suggester to match against all ...     -1   \n",
       "2     chunk document to test for plagiarism   I be b...     -1   \n",
       "3     how to use nltk regex pattern to extract a spe...     -1   \n",
       "4     itextsharp : word be break when split textchun...     -1   \n",
       "...                                                 ...    ...   \n",
       "8588  how do I set the random seed for Chroma DB ?  ...      3   \n",
       "8589  how to send a parameter directly to LLM in lan...     -1   \n",
       "8590  Azure Document Intelligence Custom Classificat...      0   \n",
       "8591  Protecting Routes in an Edge - runtime with t3...      0   \n",
       "8592  yolov5 Class Imbalance and Overfitting Issues ...     -1   \n",
       "\n",
       "                                Name  \\\n",
       "0             -1_use_model_try_error   \n",
       "1             -1_use_model_try_error   \n",
       "2             -1_use_model_try_error   \n",
       "3             -1_use_model_try_error   \n",
       "4             -1_use_model_try_error   \n",
       "...                              ...   \n",
       "8588  3_vector_document_chromadb_use   \n",
       "8589          -1_use_model_try_error   \n",
       "8590          0_api_openai_use_error   \n",
       "8591          0_api_openai_use_error   \n",
       "8592          -1_use_model_try_error   \n",
       "\n",
       "                                         Representation  \\\n",
       "0     [use, model, try, error, code, langchain, work...   \n",
       "1     [use, model, try, error, code, langchain, work...   \n",
       "2     [use, model, try, error, code, langchain, work...   \n",
       "3     [use, model, try, error, code, langchain, work...   \n",
       "4     [use, model, try, error, code, langchain, work...   \n",
       "...                                                 ...   \n",
       "8588  [vector, document, chromadb, use, chroma, stor...   \n",
       "8589  [use, model, try, error, code, langchain, work...   \n",
       "8590  [api, openai, use, error, code, response, try,...   \n",
       "8591  [api, openai, use, error, code, response, try,...   \n",
       "8592  [use, model, try, error, code, langchain, work...   \n",
       "\n",
       "                                    Representative_Docs  \\\n",
       "0     [streamlit not take more than one prompt for c...   \n",
       "1     [streamlit not take more than one prompt for c...   \n",
       "2     [streamlit not take more than one prompt for c...   \n",
       "3     [streamlit not take more than one prompt for c...   \n",
       "4     [streamlit not take more than one prompt for c...   \n",
       "...                                                 ...   \n",
       "8588  [how to retrieve id and metadata associate wit...   \n",
       "8589  [streamlit not take more than one prompt for c...   \n",
       "8590  [OpenAI API not read api key in my .env.local ...   \n",
       "8591  [OpenAI API not read api key in my .env.local ...   \n",
       "8592  [streamlit not take more than one prompt for c...   \n",
       "\n",
       "                                            Top_n_words  Probability  \\\n",
       "0     use - model - try - error - code - langchain -...     0.000000   \n",
       "1     use - model - try - error - code - langchain -...     0.000000   \n",
       "2     use - model - try - error - code - langchain -...     0.000000   \n",
       "3     use - model - try - error - code - langchain -...     0.000000   \n",
       "4     use - model - try - error - code - langchain -...     0.000000   \n",
       "...                                                 ...          ...   \n",
       "8588  vector - document - chromadb - use - chroma - ...     1.000000   \n",
       "8589  use - model - try - error - code - langchain -...     0.000000   \n",
       "8590  api - openai - use - error - code - response -...     0.898531   \n",
       "8591  api - openai - use - error - code - response -...     0.927037   \n",
       "8592  use - model - try - error - code - langchain -...     0.000000   \n",
       "\n",
       "      Representative_document  \n",
       "0                       False  \n",
       "1                       False  \n",
       "2                       False  \n",
       "3                       False  \n",
       "4                       False  \n",
       "...                       ...  \n",
       "8588                    False  \n",
       "8589                    False  \n",
       "8590                    False  \n",
       "8591                    False  \n",
       "8592                    False  \n",
       "\n",
       "[8593 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_distribution = topic_model.get_document_info(preprocessed_df['OriginalText'])\n",
    "document_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "713daef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>OriginalText</th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "      <th>Top_n_words</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Representative_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32333312</td>\n",
       "      <td>extract chunk BIO chunk sentence ? - python</td>\n",
       "      <td>extract chunk BIO chunk sentence ? - python   ...</td>\n",
       "      <td>how to extract chunk from BIO chunk sentence ?...</td>\n",
       "      <td>how to extract chunk from BIO chunk sentence ?...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_use_model_try_error</td>\n",
       "      <td>[use, model, try, error, code, langchain, work...</td>\n",
       "      <td>[streamlit not take more than one prompt for c...</td>\n",
       "      <td>use - model - try - error - code - langchain -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33695244</td>\n",
       "      <td>use Completion Suggester match ngram query</td>\n",
       "      <td>use Completion Suggester match ngram query   I...</td>\n",
       "      <td>use Completion Suggester to match against all ...</td>\n",
       "      <td>use Completion Suggester to match against all ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_use_model_try_error</td>\n",
       "      <td>[use, model, try, error, code, langchain, work...</td>\n",
       "      <td>[streamlit not take more than one prompt for c...</td>\n",
       "      <td>use - model - try - error - code - langchain -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33941091</td>\n",
       "      <td>chunk document test plagiarism</td>\n",
       "      <td>chunk document test plagiarism   I build plagi...</td>\n",
       "      <td>chunk document to test for plagiarism   I be b...</td>\n",
       "      <td>chunk document to test for plagiarism   I be b...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_use_model_try_error</td>\n",
       "      <td>[use, model, try, error, code, langchain, work...</td>\n",
       "      <td>[streamlit not take more than one prompt for c...</td>\n",
       "      <td>use - model - try - error - code - langchain -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34090734</td>\n",
       "      <td>use nltk regex pattern extract specific phrase...</td>\n",
       "      <td>use nltk regex pattern extract specific phrase...</td>\n",
       "      <td>how to use nltk regex pattern to extract a spe...</td>\n",
       "      <td>how to use nltk regex pattern to extract a spe...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_use_model_try_error</td>\n",
       "      <td>[use, model, try, error, code, langchain, work...</td>\n",
       "      <td>[streamlit not take more than one prompt for c...</td>\n",
       "      <td>use - model - try - error - code - langchain -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34318427</td>\n",
       "      <td>itextsharp : word break split textchunk word</td>\n",
       "      <td>itextsharp : word break split textchunk word  ...</td>\n",
       "      <td>itextsharp : word be break when split textchun...</td>\n",
       "      <td>itextsharp : word be break when split textchun...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_use_model_try_error</td>\n",
       "      <td>[use, model, try, error, code, langchain, work...</td>\n",
       "      <td>[streamlit not take more than one prompt for c...</td>\n",
       "      <td>use - model - try - error - code - langchain -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8588</th>\n",
       "      <td>78981951</td>\n",
       "      <td>set random seed Chroma DB ?</td>\n",
       "      <td>set random seed Chroma DB ?   I m experiment d...</td>\n",
       "      <td>how do I set the random seed for Chroma DB ?  ...</td>\n",
       "      <td>how do I set the random seed for Chroma DB ?  ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3_vector_document_chromadb_use</td>\n",
       "      <td>[vector, document, chromadb, use, chroma, stor...</td>\n",
       "      <td>[how to retrieve id and metadata associate wit...</td>\n",
       "      <td>vector - document - chromadb - use - chroma - ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8589</th>\n",
       "      <td>78982153</td>\n",
       "      <td>send parameter directly LLM langchain</td>\n",
       "      <td>send parameter directly LLM langchain   curren...</td>\n",
       "      <td>how to send a parameter directly to LLM in lan...</td>\n",
       "      <td>how to send a parameter directly to LLM in lan...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_use_model_try_error</td>\n",
       "      <td>[use, model, try, error, code, langchain, work...</td>\n",
       "      <td>[streamlit not take more than one prompt for c...</td>\n",
       "      <td>use - model - try - error - code - langchain -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8590</th>\n",
       "      <td>78984423</td>\n",
       "      <td>Azure Document Intelligence Custom Classificat...</td>\n",
       "      <td>Azure Document Intelligence Custom Classificat...</td>\n",
       "      <td>Azure Document Intelligence Custom Classificat...</td>\n",
       "      <td>Azure Document Intelligence Custom Classificat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_api_openai_use_error</td>\n",
       "      <td>[api, openai, use, error, code, response, try,...</td>\n",
       "      <td>[OpenAI API not read api key in my .env.local ...</td>\n",
       "      <td>api - openai - use - error - code - response -...</td>\n",
       "      <td>0.898531</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8591</th>\n",
       "      <td>78984512</td>\n",
       "      <td>protect Routes Edge - Runtime t3 Stack / Verce...</td>\n",
       "      <td>protect Routes Edge - Runtime t3 Stack / Verce...</td>\n",
       "      <td>Protecting Routes in an Edge - runtime with t3...</td>\n",
       "      <td>Protecting Routes in an Edge - runtime with t3...</td>\n",
       "      <td>0</td>\n",
       "      <td>0_api_openai_use_error</td>\n",
       "      <td>[api, openai, use, error, code, response, try,...</td>\n",
       "      <td>[OpenAI API not read api key in my .env.local ...</td>\n",
       "      <td>api - openai - use - error - code - response -...</td>\n",
       "      <td>0.927037</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8592</th>\n",
       "      <td>78985031</td>\n",
       "      <td>yolov5 Class Imbalance Overfitting issue</td>\n",
       "      <td>yolov5 Class Imbalance Overfitting Issues   I ...</td>\n",
       "      <td>yolov5 Class Imbalance and Overfitting Issues ...</td>\n",
       "      <td>yolov5 Class Imbalance and Overfitting Issues ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_use_model_try_error</td>\n",
       "      <td>[use, model, try, error, code, langchain, work...</td>\n",
       "      <td>[streamlit not take more than one prompt for c...</td>\n",
       "      <td>use - model - try - error - code - langchain -...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8593 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id                                              Title  \\\n",
       "0     32333312        extract chunk BIO chunk sentence ? - python   \n",
       "1     33695244         use Completion Suggester match ngram query   \n",
       "2     33941091                     chunk document test plagiarism   \n",
       "3     34090734  use nltk regex pattern extract specific phrase...   \n",
       "4     34318427       itextsharp : word break split textchunk word   \n",
       "...        ...                                                ...   \n",
       "8588  78981951                        set random seed Chroma DB ?   \n",
       "8589  78982153              send parameter directly LLM langchain   \n",
       "8590  78984423  Azure Document Intelligence Custom Classificat...   \n",
       "8591  78984512  protect Routes Edge - Runtime t3 Stack / Verce...   \n",
       "8592  78985031           yolov5 Class Imbalance Overfitting issue   \n",
       "\n",
       "                                                   Text  \\\n",
       "0     extract chunk BIO chunk sentence ? - python   ...   \n",
       "1     use Completion Suggester match ngram query   I...   \n",
       "2     chunk document test plagiarism   I build plagi...   \n",
       "3     use nltk regex pattern extract specific phrase...   \n",
       "4     itextsharp : word break split textchunk word  ...   \n",
       "...                                                 ...   \n",
       "8588  set random seed Chroma DB ?   I m experiment d...   \n",
       "8589  send parameter directly LLM langchain   curren...   \n",
       "8590  Azure Document Intelligence Custom Classificat...   \n",
       "8591  protect Routes Edge - Runtime t3 Stack / Verce...   \n",
       "8592  yolov5 Class Imbalance Overfitting Issues   I ...   \n",
       "\n",
       "                                           OriginalText  \\\n",
       "0     how to extract chunk from BIO chunk sentence ?...   \n",
       "1     use Completion Suggester to match against all ...   \n",
       "2     chunk document to test for plagiarism   I be b...   \n",
       "3     how to use nltk regex pattern to extract a spe...   \n",
       "4     itextsharp : word be break when split textchun...   \n",
       "...                                                 ...   \n",
       "8588  how do I set the random seed for Chroma DB ?  ...   \n",
       "8589  how to send a parameter directly to LLM in lan...   \n",
       "8590  Azure Document Intelligence Custom Classificat...   \n",
       "8591  Protecting Routes in an Edge - runtime with t3...   \n",
       "8592  yolov5 Class Imbalance and Overfitting Issues ...   \n",
       "\n",
       "                                               Document  Topic  \\\n",
       "0     how to extract chunk from BIO chunk sentence ?...     -1   \n",
       "1     use Completion Suggester to match against all ...     -1   \n",
       "2     chunk document to test for plagiarism   I be b...     -1   \n",
       "3     how to use nltk regex pattern to extract a spe...     -1   \n",
       "4     itextsharp : word be break when split textchun...     -1   \n",
       "...                                                 ...    ...   \n",
       "8588  how do I set the random seed for Chroma DB ?  ...      3   \n",
       "8589  how to send a parameter directly to LLM in lan...     -1   \n",
       "8590  Azure Document Intelligence Custom Classificat...      0   \n",
       "8591  Protecting Routes in an Edge - runtime with t3...      0   \n",
       "8592  yolov5 Class Imbalance and Overfitting Issues ...     -1   \n",
       "\n",
       "                                Name  \\\n",
       "0             -1_use_model_try_error   \n",
       "1             -1_use_model_try_error   \n",
       "2             -1_use_model_try_error   \n",
       "3             -1_use_model_try_error   \n",
       "4             -1_use_model_try_error   \n",
       "...                              ...   \n",
       "8588  3_vector_document_chromadb_use   \n",
       "8589          -1_use_model_try_error   \n",
       "8590          0_api_openai_use_error   \n",
       "8591          0_api_openai_use_error   \n",
       "8592          -1_use_model_try_error   \n",
       "\n",
       "                                         Representation  \\\n",
       "0     [use, model, try, error, code, langchain, work...   \n",
       "1     [use, model, try, error, code, langchain, work...   \n",
       "2     [use, model, try, error, code, langchain, work...   \n",
       "3     [use, model, try, error, code, langchain, work...   \n",
       "4     [use, model, try, error, code, langchain, work...   \n",
       "...                                                 ...   \n",
       "8588  [vector, document, chromadb, use, chroma, stor...   \n",
       "8589  [use, model, try, error, code, langchain, work...   \n",
       "8590  [api, openai, use, error, code, response, try,...   \n",
       "8591  [api, openai, use, error, code, response, try,...   \n",
       "8592  [use, model, try, error, code, langchain, work...   \n",
       "\n",
       "                                    Representative_Docs  \\\n",
       "0     [streamlit not take more than one prompt for c...   \n",
       "1     [streamlit not take more than one prompt for c...   \n",
       "2     [streamlit not take more than one prompt for c...   \n",
       "3     [streamlit not take more than one prompt for c...   \n",
       "4     [streamlit not take more than one prompt for c...   \n",
       "...                                                 ...   \n",
       "8588  [how to retrieve id and metadata associate wit...   \n",
       "8589  [streamlit not take more than one prompt for c...   \n",
       "8590  [OpenAI API not read api key in my .env.local ...   \n",
       "8591  [OpenAI API not read api key in my .env.local ...   \n",
       "8592  [streamlit not take more than one prompt for c...   \n",
       "\n",
       "                                            Top_n_words  Probability  \\\n",
       "0     use - model - try - error - code - langchain -...     0.000000   \n",
       "1     use - model - try - error - code - langchain -...     0.000000   \n",
       "2     use - model - try - error - code - langchain -...     0.000000   \n",
       "3     use - model - try - error - code - langchain -...     0.000000   \n",
       "4     use - model - try - error - code - langchain -...     0.000000   \n",
       "...                                                 ...          ...   \n",
       "8588  vector - document - chromadb - use - chroma - ...     1.000000   \n",
       "8589  use - model - try - error - code - langchain -...     0.000000   \n",
       "8590  api - openai - use - error - code - response -...     0.898531   \n",
       "8591  api - openai - use - error - code - response -...     0.927037   \n",
       "8592  use - model - try - error - code - langchain -...     0.000000   \n",
       "\n",
       "      Representative_document  \n",
       "0                       False  \n",
       "1                       False  \n",
       "2                       False  \n",
       "3                       False  \n",
       "4                       False  \n",
       "...                       ...  \n",
       "8588                    False  \n",
       "8589                    False  \n",
       "8590                    False  \n",
       "8591                    False  \n",
       "8592                    False  \n",
       "\n",
       "[8593 rows x 12 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_df = pd.concat([preprocessed_df, document_distribution], axis=1)\n",
    "concatenated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9aa18d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  0,  6,  2,  3,  4,  5,  7])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_topics = concatenated_df['Topic'].unique()\n",
    "distinct_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5c3078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in distinct_topics:\n",
    "    df_topic = concatenated_df[concatenated_df['Topic'] == topic]\n",
    "    filename = f\"/home/kha060/PhD/Developer Challenges LLM/Stack_Overflow/GeneratedTopics/{topic}_Topics_data.csv\"\n",
    "    df_topic.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bfba4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BTI",
   "language": "python",
   "name": "bti"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
